{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4025afca",
   "metadata": {},
   "source": [
    "## Downloading dependendensies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34698b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run these commands in the parent directory to this file, then cd to frank/,\n",
    "# and run the rest of the code in the notebook from there.\n",
    "\n",
    "# ! git clone https://github.com/artidoro/frank.git\n",
    "# ! pip install -r frank/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e8c27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install transformers\n",
    "! pip3 install sentencepiece\n",
    "! pip3 install protobuf==3.20.0\n",
    "! pip3 install torch==1.12.0 --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "! pip3 install nvidia-pyindex\n",
    "! pip3 install nvidia-cudnn\n",
    "! pip install evaluate\n",
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79587ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/alkobakalova/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a513e0a",
   "metadata": {},
   "source": [
    "## Benchmarking models for relativity to input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cce5985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/benchmark_data.json\", \"r\") as file:\n",
    "    benchmark_data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-welding",
   "metadata": {},
   "source": [
    "Choose on of the two models below for benchmarking. Experimentally, roberta produces better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dae467ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\")\n",
    "\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# \n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"ynie/xlnet-large-cased-snli_mnli_fever_anli_R1_R2_R3-nli\")\n",
    "# \n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"ynie/xlnet-large-cased-snli_mnli_fever_anli_R1_R2_R3-nli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-adjustment",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "label_to_explanation = {0: \"entailment\", 1: \"neutral\", 2 : \"contradiction\"}\n",
    "device = \"cuda:0\"\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e72769f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "for item in benchmark_data[len(results):]:\n",
    "    article_sents = sent_tokenize(item[\"article\"])\n",
    "    summary_sents = sent_tokenize(item[\"summary\"])\n",
    "    entailment_scores = []\n",
    "    neutral_scores = []\n",
    "    contradiction_scores = []\n",
    "    for s_sent in summary_sents:\n",
    "        entailment_scores.append([])\n",
    "        neutral_scores.append([])\n",
    "        contradiction_scores.append([])\n",
    "        for a_sent in article_sents:\n",
    "            tokenized_input_seq_pair = tokenizer.encode_plus(a_sent, s_sent,\n",
    "                                                     max_length=256,\n",
    "                                                     return_token_type_ids=True, truncation=True)\n",
    "            input_ids = torch.Tensor(tokenized_input_seq_pair['input_ids']).long().unsqueeze(0)\n",
    "            token_type_ids = torch.Tensor(tokenized_input_seq_pair['token_type_ids']).long().unsqueeze(0)\n",
    "            attention_mask = torch.Tensor(tokenized_input_seq_pair['attention_mask']).long().unsqueeze(0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids.to(device),\n",
    "                                attention_mask=attention_mask.to(device),\n",
    "                                token_type_ids=token_type_ids.to(device),\n",
    "                                labels=None)\n",
    "            scores = torch.softmax(outputs[0], dim=1)[0].tolist()\n",
    "            entailment_scores[-1].append(scores[0])\n",
    "            neutral_scores[-1].append(scores[1])\n",
    "            contradiction_scores[-1].append(scores[2])\n",
    "    results.append({\n",
    "        \"article\" : item[\"article\"],\n",
    "        \"summary\": item[\"summary\"],\n",
    "        \"hash\" : item[\"hash\"],\n",
    "        \"reference\" : item[\"reference\"],\n",
    "        \"model_name\" : item[\"model_name\"],\n",
    "        \"split\" : item[\"split\"],\n",
    "        \"entailment_scores\" : entailment_scores,\n",
    "        \"neutral_scores\": neutral_scores,\n",
    "        \"contradiction_scores\": contradiction_scores\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0abcd1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A 1 indicates that there was no such errors in the summary, a 0 indicates that every sentence contained one such error\n",
    "# scores = [[per article] per summary]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def agg_max(res):\n",
    "    res[\"score\"] = max([1 - score for scores_ in res[\"contradiction_scores\"] for score in scores_])\n",
    "    return res\n",
    "                \n",
    "def agg_min(res):\n",
    "    res[\"score\"] = min([1 - score for scores_ in res[\"contradiction_scores\"] for score in scores_])\n",
    "    return res\n",
    "\n",
    "def agg_avg(res):\n",
    "    scores = [1 - score for scores_ in res[\"contradiction_scores\"] for score in scores_]\n",
    "    res[\"score\"] = sum(scores) / len(scores)\n",
    "    return res\n",
    "\n",
    "def true_agg(res):\n",
    "    scores = [1 - max([score for score in scores_]) for scores_ in res[\"contradiction_scores\"]]\n",
    "    res[\"score\"] = sum(scores) / len(scores)\n",
    "    return res\n",
    "\n",
    "def true_agg_r(res):\n",
    "    scores_r = np.array(res[\"contradiction_scores\"]).T\n",
    "    scores = [1 - max([score for score in scores_]) for scores_ in scores_r]\n",
    "    res[\"score\"] = sum(scores) / len(scores)\n",
    "    return res\n",
    "\n",
    "def true_agg_2(res):\n",
    "    scores = [min([1 - score for score in scores_]) for scores_ in res[\"contradiction_scores\"]]\n",
    "    res[\"score\"] = sum(scores) / len(scores)\n",
    "    return res\n",
    "\n",
    "def true_agg_2_r(res):\n",
    "    scores_r = np.array(res[\"contradiction_scores\"]).T\n",
    "    scores = [min([1 - score for score in scores_]) for scores_ in scores_r]\n",
    "    res[\"score\"] = sum(scores) / len(scores)\n",
    "    return res\n",
    "\n",
    "def agg_contradiction_max(res):\n",
    "    res[\"score\"] = 1 - max([score for scores_ in res[\"contradiction_scores\"] for score in scores_])\n",
    "    return res\n",
    "                \n",
    "def agg_contradiction_min(res):\n",
    "    res[\"score\"] = 1 - min([score for scores_ in res[\"contradiction_scores\"] for score in scores_])\n",
    "    return res\n",
    "\n",
    "def agg_contradiction_avg(res):\n",
    "    scores = [score for scores_ in res[\"contradiction_scores\"] for score in scores_]\n",
    "    res[\"score\"] = 1 - sum(scores) / len(scores)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcf30a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "agg_foos = [agg_avg, agg_min, agg_max, agg_contradiction_max, agg_contradiction_min, agg_contradiction_avg, true_agg,\n",
    "            true_agg_2, true_agg_r, true_agg_2_r]\n",
    "agg_foos_names = [\"agg_avg\", \"agg_min\", \"agg_max\", \"agg_contradiction_max\", \"agg_contradiction_min\",\n",
    "                  \"agg_contradiction_avg\", \"true_agg\", \"true_agg_2\", \"true_agg_r\", \"true_agg_2_r\"]\n",
    "\n",
    "agg_results = {}\n",
    "for agg_foo, name in zip(agg_foos, agg_foos_names):\n",
    "    agg_results[name] = [agg_foo(res) for res in results]\n",
    "    with open(f\"results/roberta-{name}-results.json\", \"w\") as file:\n",
    "        json.dump(agg_results[name], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca27fa87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/evaluate.py:6: DeprecationWarning: Please use `pearsonr` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
      "  from scipy.stats.stats import pearsonr, spearmanr\n",
      "evaluation/evaluate.py:6: DeprecationWarning: Please use `spearmanr` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
      "  from scipy.stats.stats import pearsonr, spearmanr\n",
      "Info: metric Bleu used 375 summaries to calculate correlation.\n",
      "Info: metric Meteor used 375 summaries to calculate correlation.\n",
      "Info: metric Rouge 1 used 375 summaries to calculate correlation.\n",
      "Info: metric Rouge 2 used 375 summaries to calculate correlation.\n",
      "Info: metric Rouge L used 375 summaries to calculate correlation.\n",
      "Info: metric BertScore P Art used 375 summaries to calculate correlation.\n",
      "Info: metric FEQA used 375 summaries to calculate correlation.\n",
      "Info: metric QAGS used 375 summaries to calculate correlation.\n",
      "Info: metric Dep Entail used 339 summaries to calculate correlation.\n",
      "Info: metric FactCC used 375 summaries to calculate correlation.\n",
      "Info: metric score used 375 summaries to calculate correlation.\n",
      "                  pearson  pearson p-value  spearman  spearman p-value\n",
      "Bleu             0.115828     2.489224e-02  0.059618      2.494614e-01\n",
      "Meteor           0.145437     4.772520e-03  0.073358      1.562692e-01\n",
      "Rouge 1          0.160672     1.800428e-03  0.102423      4.747861e-02\n",
      "Rouge 2          0.130462     1.144655e-02  0.059611      2.495093e-01\n",
      "Rouge L          0.131514     1.079305e-02  0.059277      2.521789e-01\n",
      "BertScore P Art  0.278269     4.271011e-08  0.246000      1.420020e-06\n",
      "FEQA            -0.023975     6.435213e-01 -0.022708      6.611446e-01\n",
      "QAGS             0.046926     3.648338e-01  0.034941      4.999502e-01\n",
      "Dep Entail       0.174094     1.290070e-03  0.074330      1.721280e-01\n",
      "FactCC           0.362065     4.659342e-13  0.373939      6.856268e-14\n",
      "score            0.272850     7.949518e-08  0.232517      5.360762e-06\n"
     ]
    }
   ],
   "source": [
    "! python evaluation/evaluate.py --metrics_outputs \"results/roberta-true_agg_2_r-results.json\" --dataset cnndm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38799e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation/evaluate.py:6: DeprecationWarning: Please use `pearsonr` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
      "  from scipy.stats.stats import pearsonr, spearmanr\n",
      "evaluation/evaluate.py:6: DeprecationWarning: Please use `spearmanr` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n",
      "  from scipy.stats.stats import pearsonr, spearmanr\n",
      "Info: metric Bleu used 375 summaries to calculate correlation.\n",
      "Info: metric Meteor used 375 summaries to calculate correlation.\n",
      "Info: metric Rouge 1 used 375 summaries to calculate correlation.\n",
      "Info: metric Rouge 2 used 375 summaries to calculate correlation.\n",
      "Info: metric Rouge L used 375 summaries to calculate correlation.\n",
      "Info: metric BertScore P Art used 375 summaries to calculate correlation.\n",
      "Info: metric FEQA used 375 summaries to calculate correlation.\n",
      "Info: metric QAGS used 375 summaries to calculate correlation.\n",
      "Info: metric Dep Entail used 339 summaries to calculate correlation.\n",
      "Info: metric FactCC used 375 summaries to calculate correlation.\n",
      "Info: metric score used 375 summaries to calculate correlation.\n",
      "                  pearson  pearson p-value  spearman  spearman p-value\n",
      "Bleu             0.115828     2.489224e-02  0.059618      2.494614e-01\n",
      "Meteor           0.145437     4.772520e-03  0.073358      1.562692e-01\n",
      "Rouge 1          0.160672     1.800428e-03  0.102423      4.747861e-02\n",
      "Rouge 2          0.130462     1.144655e-02  0.059611      2.495093e-01\n",
      "Rouge L          0.131514     1.079305e-02  0.059277      2.521789e-01\n",
      "BertScore P Art  0.278269     4.271011e-08  0.246000      1.420020e-06\n",
      "FEQA            -0.023975     6.435213e-01 -0.022708      6.611446e-01\n",
      "QAGS             0.046926     3.648338e-01  0.034941      4.999502e-01\n",
      "Dep Entail       0.174094     1.290070e-03  0.074330      1.721280e-01\n",
      "FactCC           0.362065     4.659342e-13  0.373939      6.856268e-14\n",
      "score            0.272850     7.949518e-08  0.232517      5.360762e-06\n"
     ]
    }
   ],
   "source": [
    "! python evaluation/evaluate.py --metrics_outputs \"results/roberta-true_agg_r-results.json\" --dataset cnndm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a4cb997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Info: metric Bleu used 375 summaries to calculate correlation.\n",
      "Info: metric Meteor used 375 summaries to calculate correlation.\n",
      "Info: metric Rouge 1 used 375 summaries to calculate correlation.\n",
      "Info: metric Rouge 2 used 375 summaries to calculate correlation.\n",
      "Info: metric Rouge L used 375 summaries to calculate correlation.\n",
      "Info: metric BertScore P Art used 375 summaries to calculate correlation.\n",
      "Info: metric FEQA used 375 summaries to calculate correlation.\n",
      "Info: metric QAGS used 375 summaries to calculate correlation.\n",
      "Info: metric Dep Entail used 339 summaries to calculate correlation.\n",
      "Info: metric FactCC used 375 summaries to calculate correlation.\n",
      "Info: metric score used 375 summaries to calculate correlation.\n",
      "                  pearson  pearson p-value  spearman  spearman p-value\n",
      "Bleu             0.115828     2.489224e-02  0.059618      2.494614e-01\n",
      "Meteor           0.145437     4.772520e-03  0.073358      1.562692e-01\n",
      "Rouge 1          0.160672     1.800428e-03  0.102423      4.747861e-02\n",
      "Rouge 2          0.130462     1.144655e-02  0.059611      2.495093e-01\n",
      "Rouge L          0.131514     1.079305e-02  0.059277      2.521789e-01\n",
      "BertScore P Art  0.278269     4.271011e-08  0.246000      1.420020e-06\n",
      "FEQA            -0.023975     6.435213e-01 -0.022708      6.611446e-01\n",
      "QAGS             0.046926     3.648338e-01  0.034941      4.999502e-01\n",
      "Dep Entail       0.174094     1.290070e-03  0.074330      1.721280e-01\n",
      "FactCC           0.362065     4.659342e-13  0.373939      6.856268e-14\n",
      "score            0.233725     4.774161e-06  0.199264      1.023821e-04\n"
     ]
    }
   ],
   "source": [
    "! python evaluation/evaluate.py --metrics_outputs \"xlnet-max-results.json\" --dataset cnndm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
