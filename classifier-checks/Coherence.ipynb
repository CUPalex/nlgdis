{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "still-balloon",
   "metadata": {},
   "source": [
    "## Define scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-roman",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch, tqdm, logging\n",
    "import numpy as np\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\", return_dict=True).to(device).eval()\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8777b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_loss(logits, inputs, attention_mask):\n",
    "    labels = torch.nn.functional.one_hot(inputs.clone(), num_classes=50257)\n",
    "    labels = (labels.permute(2, 0, 1) * attention_mask).permute(1, 2, 0)\n",
    "    logits = (logits.permute(2, 0, 1) * attention_mask).permute(1, 2, 0)\n",
    "    return  torch.log((torch.nn.functional.softmax(logits[:, :-1, :], dim=-1) * labels[:, 1:, :]).sum(dim=-1)\n",
    "                     + (1 - attention_mask[:, 1:])).sum(dim=-1) / attention_mask.sum(dim=-1)\n",
    "\n",
    "def perplexity(generated_sents):\n",
    "    # does not use input_sent\n",
    "    max_length = model.config.n_positions # 1024\n",
    "    inputs = tokenizer(\n",
    "        generated_sents, max_length=max_length, truncation=True, padding=True, return_tensors='pt'\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs[\"input_ids\"].to(device))\n",
    "        return count_loss(\n",
    "            outputs.logits.cpu(), inputs[\"input_ids\"].cpu(), inputs[\"attention_mask\"]).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8798810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perplexity_per_doc(model, tokenizer, doc):\n",
    "    max_length = model.config.n_positions\n",
    "    stride=512\n",
    "\n",
    "    inputs = tokenizer(doc, return_tensors='pt')\n",
    "    lls = []\n",
    "    for i in range(0, inputs['input_ids'].size(1), stride):\n",
    "        # getting the coordinates of the window\n",
    "        begin_loc = max(i + stride - max_length, 0)\n",
    "        end_loc = min(i + stride, inputs['input_ids'].size(1))\n",
    "        target_len = end_loc - i\n",
    "\n",
    "        input_ids = inputs['input_ids'][:, begin_loc:end_loc]\n",
    "        target_ids = input_ids.clone()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "            log_likelihood = outputs.loss * target_len\n",
    "        lls.append(log_likelihood)\n",
    "    return torch.exp(torch.stack(lls).sum() / end_loc).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-species",
   "metadata": {},
   "source": [
    "## Check correlation with GCDC data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-tennis",
   "metadata": {},
   "source": [
    "You need to download GCDC dataset first. To get access to the data, please contact the authors of the GCDC paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "255978ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "perplexities = []\n",
    "test_dataset = pd.read_csv(\"../GCDC_rerelease/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93262f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in test_dataset.iterrows():\n",
    "    perplexities.append(perplexity(row[1].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "future-marsh",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-3.3810136], dtype=float32), array([-3.2851007], dtype=float32))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.quantile(perplexities, 0.25), np.quantile(perplexities, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "liquid-palestine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47375"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_th(p):\n",
    "    if p < np.quantile(perplexities, 0.2):\n",
    "        return 1\n",
    "    if p < np.quantile(perplexities, 0.3):\n",
    "        return 2\n",
    "    return 3\n",
    "classified = [get_th(p) for p in perplexities]\n",
    "acc = (np.array(list(test_dataset.label)) == np.array(classified[800:])).mean(); acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "serial-cooler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28 0.48375\n",
      "0.3 0.47375\n",
      "0.32 0.4675\n",
      "0.34 0.44875\n"
     ]
    }
   ],
   "source": [
    "prev_acc = 0.34\n",
    "for b_2 in [0.28, 0.3, 0.32, 0.34]:\n",
    "    def get_th(p):\n",
    "        if p < np.quantile(perplexities, 0.2):\n",
    "            return 1\n",
    "        if p < np.quantile(perplexities, b_2):\n",
    "            return 2\n",
    "        return 3\n",
    "    classified = [get_th(p) for p in perplexities[800:]]\n",
    "    acc = (np.array(list(test_dataset.label)) == np.array(classified)).mean()\n",
    "    print(b_2, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "extraordinary-mailing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3 0.52375\n"
     ]
    }
   ],
   "source": [
    "prev_acc = 0.51375\n",
    "for b_2 in [0.3]:\n",
    "    def get_th(p):\n",
    "        if p < np.quantile(perplexities, 0.2):\n",
    "            return 1\n",
    "        if p < np.quantile(perplexities, b_2):\n",
    "            return 2\n",
    "        return 3\n",
    "    classified = [get_th(p) for p in perplexities]\n",
    "    acc = (np.array(list(test_dataset.label)) == np.array(classified)).mean()\n",
    "    print(b_2, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "perceived-albuquerque",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51375"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array(list(test_dataset.label)) == np.array(classified)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3755d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexities = [perp[0] for perp in perplexities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "864c68a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignificanceResult(statistic=0.32728579266288776, pvalue=1.9892314863379266e-21)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "spearmanr(perplexities, list(test_dataset.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58cd0de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PearsonRResult(statistic=0.33584569962759214, pvalue=1.5225851221352634e-22)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "pearsonr(perplexities, list(test_dataset.label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
